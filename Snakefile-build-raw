"""
Snakefile for doing the first stages of data processing from the daq sandbox files
to the blinded raw data. It handles:
- moving the daq files from the sandbox to the sorted file system
- running build raw on this data (with trimming)
- blinding the physics data
"""

import pathlib, os, sys
from scripts.util.patterns import (
    get_pattern_unsorted_data,
    get_pattern_tier_daq,
)
from scripts.util.utils import (
    subst_vars_in_snakemake_config,
    runcmd,
    config_path,
    chan_map_path,
    filelist_path,
    pars_path,
    metadata_path,
    det_status_path,
)
import scripts.util as ds

check_in_cycle = True

# Set with `snakemake --configfile=/path/to/your/config.json`
# configfile: "have/to/specify/path/to/your/config.json"

subst_vars_in_snakemake_config(workflow, config)

setup = config["setups"]["l200"]
configs = config_path(setup)
chan_maps = chan_map_path(setup)
swenv = runcmd(setup)
meta = metadata_path(setup)
det_status = det_status_path(setup)

basedir = workflow.basedir


wildcard_constraints:
    experiment=r"\w+",
    period=r"p\d{2}",
    run=r"r\d{3}",
    datatype=r"\w{3}",
    timestamp=r"\d{8}T\d{6}Z",


localrules:
    gen_filelist,
    autogen_output,


# raw_par_catalog = ds.pars_key_resolve.get_par_catalog(
#     ["-*-*-*-cal"],
#     [
#         get_pattern_unsorted_data(setup),
#         get_pattern_tier_daq(setup),
#         get_pattern_tier(setup, "raw"),
#     ],
#     {"cal": ["par_raw"]},
# )


onstart:
    print("Starting workflow")

    # raw_par_cat_file = os.path.join(pars_path(setup), "raw", "validity.jsonl")
    # if os.path.isfile(raw_par_cat_file):
    #     os.remove(os.path.join(pars_path(setup), "raw", "validity.jsonl"))
    # pathlib.Path(os.path.dirname(raw_par_cat_file)).mkdir(parents=True, exist_ok=True)
    # ds.pars_key_resolve.write_to_jsonl(raw_par_catalog, raw_par_cat_file)



onsuccess:
    print("Workflow finished, no error")
    shell("rm *.gen || true")
    shell(f"rm {filelist_path(setup)}/* || true")


include: "rules/common.smk"
include: "rules/filelist_gen.smk"
include: "rules/main.smk"
include: "rules/raw.smk"
include: "rules/blinding_check.smk"


# FIXME: cannot put extension="*", otherwise it won't be possible to extract
# keys (see FileKey.get_path_from_filekey())
rule gen_filelist:
    input:
        lambda wildcards: get_filelist(
            wildcards,
            setup,
            get_pattern_unsorted_data(setup, extension="fcio"),
            ignore_keys_file=Path(det_status) / "ignored_daq_cycles.yaml",
            analysis_runs_file=Path(det_status) / "runlists.yaml",
        ),
    output:
        temp(Path(filelist_path(setup)) / "{label}-{tier}.filelist"),
    run:
        print(f"INFO: found {len(input)} files")
        if len(input) == 0:
            print(
                f"WARNING: no DAQ files found for the given pattern: {wildcards.label}. "
                "make sure patterns follows the format: "
                "all-{experiment}-{period}-{run}-{datatype}-{timestamp}-{tier}.gen"
            )
        with open(output[0], "w") as f:
            for fn in input:
                f.write(f"{fn}\n")


rule sort_data:
    """
    This rules moves the daq data from the unsorted sandbox dir
    to the sorted dirs under generated
    """
    input:
        get_pattern_unsorted_data(setup, extension="fcio"),
    output:
        get_pattern_tier_daq(setup, extension="fcio"),
    shell:
        "mv {input} {output}"


# vim: ft=snakemake
